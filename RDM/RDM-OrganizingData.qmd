---
title: Principles for Organizing research data

bibliography: references.bib
---

## Make datasets understandable

::: {layout-ncol="2"}
::: {#first-column}
Research data comes in many tastes and shapes (tables, images, videos, text). To facilitate collaboration and future reuse, datasets must be clear and easy to understand, not only by the original researchers, but specially by others who may encounter the data in the future.

The preceding implies [systematic procedures in data organization]{style="color:green;"}, that is, datasets should be structured logically, with labels that make them easy to understand. The files and variables, and their corresponding relationships must be clear enough by [external users]{style="color:green;"}.
:::

::: {#second-column}
::: {style="text-align: center; font-size: 100%"}
![Others generally do not understand research raw data](images/DificultData.webp){fig-align="center" width="250" height="250"}
:::
:::
:::

::: callout-tip
Try to put yourself in the shoes of an [outside observer]{style="color:green;"} when structuring the data.
:::

## Best practices for organizing a dataset

Organizing research data effectively is crucial for ensuring data management and producle reproducible science. Well-structured files and folders, along with consistent and meaningful naming conventions, can save significant time and effort in understanding the data. Below are listed some key principles for achieving an effective data organization:

### <i class="bi bi-file-earmark-font"></i> Use consistent [naming conventions]{style="color:green;"}

A consistent and logical file naming convention allows researchers identify and easily retrieve files. Lacking a clear and consistent naming system put at risk the location of specific files or, worse, losing track of important data. Specifically, good naming practices ensure that files are easily understood and unambiguous to anyone who accesses them.

Here are five relevant aspects for naming files:

-   **Be descriptive yet concise:** In principle, file names should describe the content, date, or purpose of the file but should not be excessively long. The goal main goal is to provide logical information so that someone unfamiliar can understand what the file contains. Long names are difficult to understand and might generate problems when the complete file route is over 256 characters.

::: callout-tip
## Example

Instead of `data1.csv`, a research can name the file `climate_data_Jan2024.csv`, which specifies it is climate data collected in January 2024.
:::

-   **Avoid special characters:** Some operating systems or software may not handle special characters in file names or generate unwanted workflows. It is recommended to [avoid spaces]{style="color:red;"}, slashes (/), backslashes (), and symbols like !, \@, and #.

::: callout-tip
## Example

Instead of `climate_data_Jan2024#2.csv` use underscores or hyphens to indicate a characteristic or version: `climate_data_Jan2024_v2.0.csv`
:::

Example:

-   **Use leading zeros in sequential numbering:** For files that includes numeration, include leading zeros to keep the files in the correct sequence.

::: callout-tip
## Example

Use `climate_data_01.csv, climate_data_05.csv, climate_data_10.csv`, instead of `climate_data_1.csv, climate_data_5.csv, climate_data_10.csv`
:::

-   **Include versioning information (whe sutitable):** If your research plan envisages that data files may be updated or modified over time, include version numbers in the file name is useful. This facilitates identification of different versions and prevents accidental overwriting.

::: callout-tip
## Example

Use `survey_results_v1.0.csv, survey_results_v2.0.csv`
:::

-   **Indicate date formats clearly:** When date is a relevant variable, always follow a standard format such as YYYY-MM-DD (year-month-day). This is the internationally recommended format and avoids confusion.

::: callout-tip
## Example

experiment_notes_2024-01-15.docx instead of experiment_notes_15-01-2024.docx (which could cause confusion between day-month-year and month-day-year formats).
:::

### <i class="bi bi-card-image"></i> Use proper, accesible file formats

Choosing the right file formats for research data is essential in ensuring that the data remains accessible and widely reusable in the short term and over the long term. The choice of file format affects how easily data can be shared and re-used by other researchers. It is recommended to use open software tools and non-proprietary to increase the sustainability of research data.

Files formats depend on the nature of the data and how it will be used. Common examples are:

#### a. Textual data

-   **Tabular data (.csv, .xls/.xlsx):** CSV (Comma-Separated Values) (i.e `dataset.csv`) is one of the most widely used (and preferred) formats for tabular data. This format is simple, lightweight, and supported by all data analysis software including spreadsheet programs (like Excel) and open statistical software (like R or Python). Other sorts of tabular data are Excel (.xls/.xlsx) files, widely used for storing spreadsheet data. Contrary to .csv files, Excel files support multiple sheets, complex formulas, and data visualization. However, they are a proprietary format, and long-term accessibility could be a concern if Excel becomes outdated (although unlikely). It is recommended to use non-proprietary formats like .csv or .tsv (tab-separated values) for long-term storage.

-   **Plain text:** Plain text files (i.e `notes.txt`) are used for simple text data without any formatting. This approach is universally readable and have no dependencies on proprietary software.

#### b. Geospatial data

-   **GeoTIFF (.tiff):** This format (i.e `map.tiff`) is a widely used format for storing raster geospatial data, including satellite imagery or digital elevation models. This format store not only the image data but also valuable georeferencing information, allowing the information to be used in geographic information systems (GIS).

-   **Shapefile (.shp/.dbf/.shx):** This format (i.e `study_area.shp`) is a common vector data format in GIS that stores geographic features like points, lines, and polygons. This format is widely supported, but consists of multiple related files, which must all be preserved together.

#### c. Image Data

-   **TIFF (Tagged Image File Format):** TIFF is a non-proprietary format (i.e `microscopy_image.tiff`) for storing high-quality images. It is widely used in fields like microscopy, medical imaging, and remote sensing, as it supports lossless compression, ensuring that no data is lost when saving the file. Importantly, this format stores relevant image metadata.

-   **PNG (Portable Network Graphics):** This is a non-compressed image format (i.e `graph.png`) that is widely used for graphs. They are usually smaller than TIFFs, meaning some image quality is sacrificed in exchange for reduced file size.Different to TIFF, they do not store image metadata.

#### d. Audio and Video Data

-   **WAV (Waveform Audio File Format):** This is a widely used format (i.e `audio.wav`) for storing uncompressed audio data, offering high fidelity. It is suitable for research audio files that requires precise data without compression.

-   **MP4 (MPEG-4):** This is a popular format (i.e `experiment_video.mp4`) for video data that balances file size and quality using lossy compression. MP4 is commonly used for videos and is widely compatible with many devices.

#### e. Code files

-   **Python (.py):** Python is an open-source language, and .py files are plain text (i.e `analysis_script.py`), making them universally accessible and easy to share. They are widely used for data analysis, machine learning, and automation.

-   **R (.R):** R scripts are used for statistical analysis and data visualization (i.e `data_processing_script.R`). They are similar to Python scripts in that they are plain text and highly portable within the R environment.

-   **Quarto (.qmd):** Quarto files are written in markdown syntax and can run Python or R code. It has multiple application including data analysis, written and web application. The sorces files foer the current website are .wmd files.

| Data Type             | Recommended Formats                 | Notes                                        |
|------------------|-------------------------|-----------------------------|
| Tabular data          | CSV, TSV                            | Widely supported, non-proprietary            |
| Text documents        | TXT, PDF/A                          | PDF/A is a long-term archiving format        |
| Images (high-quality) | TIFF, PNG                           | TIFF for lossless, PNG for smaller size      |
| Geospatial data       | GeoTIFF, Shapefile                  | Ensure all Shapefile components are included |
| Audio data            | WAV, FLAC                           | WAV for uncompressed audio                   |
| Video data            | MP4, MOV                            | MP4 is widely compatible, but lossy          |
| Code                  | Python (.py), R (.R), Quarto (.qmd) | Provide well-commented code and scripts      |

When choosing file format, consider the following:

-   **Interoperability:** Prefer formats that are widely supported across platforms and disciplines. For example, CSV files can be opened in nearly any statistical or spreadsheet software.

-   **Non-proprietary formats:** Open, non-proprietary formats (e.g., CSV, TXT, TIFF) are preferable for long-term data preservation, as they are not tied to specific software that could become obsolete. Also, they can be used for researchers without using costly software.

-   **Data integrity and fidelity:** For images, audio, and video, lossless formats like TIFF and WAV preserve data quality, whereas lossy formats like JPEG and MP4 reduce file size at the expense of some data loss. Use lossless formats when high fidelity is essential.

## Include metadata, a readme file, and relevant documentation

Metadata and proper documentation are critical components of effective RDM. Metadata is data that accurately describes the research data. This is essential to provide context and essential information to understand, access, and reuse the data. Keep in mind that proper documentation ensures that datasets are comprehensible not only to those who originally collected the data but also to other researchers who may reuse it. Most of research data is inaccessible or unusable because they are not properly documented.

Examples of metadata are:

-   **Descriptive metadata:** This information helps users find, identify, and understand the dataset. It includes details like the title of the dataset, the author(s), abstract, keywords, and a description of the contents.

::: callout-tip
## Example

Title: Climate Measurements for Coastal Regions (2023-2024) in Quebec, Canada. Author: Dr. Jane Doe, Coastal University Keywords: climate, temperature, humidity, precipitation, coastal zones, Quebec.
:::

-   **Structural metadata:** The information specifies how the dataset is organized, including data formats, and potential relationships between different parts of the data/files.

::: callout-tip
## Example

File type: CSV (comma-separated values) Structure: Columns represent temperature (°C), humidity (%), wind speed (km/h), and time (ISO 8601 format)
:::

-   **Administrative metadata:** This includes information about data management, access restrictions, licensing, and any related technical details (e.g., software used to generate the data).

::: callout-tip
## Example

License: CC BY 4.0 (Creative Commons Attribution) Access: Publicly available through Zenodo repository
:::

-   **Technical metadata:** This includes information about the parameters, equipement and sofware used to generate the data.

::: callout-tip
## Example

Microscope used for imaging: AxioZeiss Microscope A2 Software used for imaging: Zen Blue 10.2 Software used for image processing: Python Software used for data analysis: R (Quarto)
:::

This metadata information is specified in different research data repositories like [Zenodo](https://zenodo.org/), [FRDR](https://www.frdr-dfdr.ca/repo/) and [OSF](https://osf.io/) or the readme file that the user accommodates when depositing the data.

### The readme file

::: {layout-ncol="2"}
<div>

A [README]{style="color:green;"} file is a simple text file that explains the contents of a dataset. It can describe the data files, the folder structure, the available variables, and the methodology used to collect the data. This document is pivotal to contextualize and offer reuse opportunities for other uses.

**Elements in a readme file**

-   Author and affiliation
-   Data collection date
-   Description of the structure of the dataset
-   Description of files and naming conventions
-   Methodological information (how the data was generated/obtained)
-   License and access information
-   Relation with other research data

An example of a readme file can be found [here](https://g-772fa5.cd4fe.0ec8.data.globus.org/1/published/publication_850/submitted_data/README.txt)

</div>

<div>

::: {style="text-align: center;font-size: 100%"}
![Example of readme file](images/DescriptiveMetadata_czi.png){fig-alt="Screenshot showing an example of a readme file" fig-align="center" width="90%"}
:::

</div>
:::

::: callout-tip
The readme file provides a description of a dataset as standalone object, independently of associated research articles.
:::

### The codebook

::: {layout-ncol="2"}
<div>

A [CODEBOOK]{style="color:green;"} file, also called data dictionary, explain the variables and coding schemes present (generally) in a data table. Also, they codebooks describe how missing data is handled and can present other information like frequency or distribution.

**Elements of a codebook**

-   Names and description of the variables
-   Type of variable (continuous, categorical)
-   Range or levels of the variable
-   Number of missing data and its representation

A vignette to generate codebooks can be found [here](https://github.com/indepth-data/DataScience_Teaching/blob/main/resources/GeneratingCodebook_Vignette.pdf)

</div>

<div>

::: {style="text-align: center;font-size: 100%"}
![Example of a codebook](images/Codebook.png){fig-alt="Screenshot showing an example of a codebook" fig-align="center" width="80%"}
:::

</div>
:::

::: callout-tip
The readme file provides a description of a dataset as standalone object, independently of associated research articles.
:::

# Implement reproducible pipelines using code (R, python)

## A worrying research landscape

We live in a pandemic of [fraudulent and irreproducible science]{style="color:red;"} [@fang2012; @eckhartt2023]. This worrying landscape demands that as integral researchers [committed to best scientific practices]{style="color:green;"}, we share all data handling and analysis procedures. Fortunately, the optimal ways to do this are [free]{style="color:green;"} and have tremendous [support]{style="color:green;"} from a growing community ([Py Ladies](https://github.com/pyladies), [R Ladies](https://github.com/rladies)).


::: {style="text-align: center; font-size: 60%"}
![Increase in the number of retracted articles in the last three decades](images/Economist_Retractions.png){fig-alt="Illustration showing the increse in the number of retracted research articles from 1996 to 2023" fig-align="center" width=40%}
:::


## Partners to handle analysis pipelines {.center}

::::: {layout-ncol="2"}
:::: {#first-column} 
::: {style="text-align: center;font-size: 100%"}
![R-studio/quarto screen](images/R-studio_Screen.jpg){fig-alt="Screenshot showing the R-studio interface" fig-align="center" width="100%"}
:::
::::

:::: {#second-column} 

**With R and R studio you can:**

<i class="bi bi-file-earmark-spreadsheet-fill"></i> Handle [data tables]{style="color:green;"} and variables using the R Tidyverse.

<i class="bi bi-images"></i> [Analyze images]{style="color:green;"} using Python skimage.

<i class="bi bi-wallet-fill"></i> [Process Flow cytometry]{style="color:green;"} files/data using R FlowCore from BioConductor.

<i class="bi bi-bandaid"></i> [Analyze RNS-seq]{style="color:green;"} data using R DESeq2 from BioConductor.

<i class="bi bi-bar-chart-line-fill"></i> Perform state-of-the-art [statistical modeling]{style="color:green;"} using `brms` (avoiding missused/irrelevant t-test and ANOVAs).

And all other things you can imagine…
::::
:::::


## Keep version control using git

::::: {layout-ncol="2"}
:::: {#first-column} 
::: {style="text-align: center;font-size: 100%"}
![GitHub screen](images/GitHub_screen.jpg){fig-alt="Screenshot showing the GitHub website interface" fig-align="center" width="100%"}
:::
::::

:::: {#second-column} 
::: {style="text-align: left;font-size:100%"}
**With GitHub or GitLab you can:**

<i class="bi bi-database-fill-check"></i> [Store]{style="color:green;"} your code/data in a secure place and [share it]{style="color:green;"} with collaborators and the public.

<i class="bi bi-subtract"></i> Keep a [history of changes]{style="color:green;"} and version your code (v 1.0, 1.2, 2.0).

<i class="bi bi-folder-plus"></i> [Link/render]{style="color:green;"} your code in different platforms (i.e [Open Science Framework Repository](https://osf.io/)).

<i class="bi bi-clipboard2-pulse-fill"></i> With your code you [support other researchers]{style="color:green;"} and contribute to a culture of [open and reproducible science]{style="color:green;"}.
:::
::::
:::::



